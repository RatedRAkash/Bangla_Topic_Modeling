{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLSA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UgEm91sTvXHU",
        "3hPUl_bYvTS9",
        "MllxkVrLwRkS",
        "3RurYs6MuQaj",
        "gdNfvk2es769",
        "86xjsiqgw7Tu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgEm91sTvXHU"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atY4lH0CKhZ3"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSab0tKc_qzg"
      },
      "source": [
        "!pip install plsa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3uENfm9vR0K"
      },
      "source": [
        "!pip install bnlp_toolkit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iDyEVO_bUO7"
      },
      "source": [
        "!pip install bnltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYkzzb7Y_rFV"
      },
      "source": [
        "!pip install git+https://github.com/banglakit/bengali-stemmer.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hPUl_bYvTS9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SBS2qq0uRju"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "\n",
        "import plsa\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from bnlp.corpus import stopwords as stopwords_1, punctuations, digits\n",
        "from bnlp.corpus.util import remove_stopwords\n",
        "from bnltk.tokenize import Tokenizers\n",
        "from bengali_stemmer.rafikamal2014 import RafiStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7crcsGLWUCy"
      },
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook\n",
        "\n",
        "from plsa.pipeline import DEFAULT_PIPELINE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7UbLRCRwOtY"
      },
      "source": [
        "# Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTP3CSzWuWS9",
        "outputId": "a6cb4517-0923-46ae-a9ad-48559688b2c0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Q9h12U2XOr"
      },
      "source": [
        "%cd '/content/drive/My Drive/THESIS/PLSA'\n",
        "\n",
        "!pip install cache-magic\n",
        "import cache_magic\n",
        "!mkdir .cache\n",
        "!ln -s '/content/drive/My Drive/THESIS/PLSA/.cache' /content/.cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MllxkVrLwRkS"
      },
      "source": [
        "# Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "S1RY_dB7unHu",
        "outputId": "3784eb61-0030-48a6-f085-2c57df7f8fcc"
      },
      "source": [
        "#articles = pd.read_csv('/content/drive/My Drive/Bangla-Article-and-Summary.csv')\n",
        "articles = pd.read_csv('/content/drive/MyDrive/THESIS/Final Dataset/all_final_reduced.csv', error_bad_lines=False)\n",
        "print(articles.isnull().sum())\n",
        "articles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full_Text    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Full_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>শনিবার রাতে উপজেলার বামনডাঙ্গা থেকে তাদের আটক...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>লেখক, শিক্ষক, অনলাইন অ্যাকটিভিস্ট, ভিন্নমতের ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>রাত ১২টা ০১ মিনিটে নয়া পল্টনে বিএনপির কেন্দ্র...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>গোলাকার পৃথিবীর আবর্তনের কারণে নিউ জিল‌্যান্ড...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>শনিবার রাতে গুলশান-২ নম্বর গোলচত্বরে থার্টি ফ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321691</th>\n",
              "      <td>স্বাস্থ্যকর জীবনযাপনের জন্য সুষম খাবার জরুরি। ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321692</th>\n",
              "      <td>শীতে সর্দি-কাশি সমস্যা হয়ে থাকে। অনেক সময় দেখা...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321693</th>\n",
              "      <td>বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ে কর্মকর্তা নিয়...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321694</th>\n",
              "      <td>অনেকের কোথাও ঘুরতে যাওয়ার সময়, দীর্ঘক্ষণ বাসে-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321695</th>\n",
              "      <td>শীত এলেই বেড়ে যায় খুশকির সমস্যা। চুল ঝরা, রুক্...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>321696 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Full_Text\n",
              "0        শনিবার রাতে উপজেলার বামনডাঙ্গা থেকে তাদের আটক...\n",
              "1        লেখক, শিক্ষক, অনলাইন অ্যাকটিভিস্ট, ভিন্নমতের ...\n",
              "2        রাত ১২টা ০১ মিনিটে নয়া পল্টনে বিএনপির কেন্দ্র...\n",
              "3        গোলাকার পৃথিবীর আবর্তনের কারণে নিউ জিল‌্যান্ড...\n",
              "4        শনিবার রাতে গুলশান-২ নম্বর গোলচত্বরে থার্টি ফ...\n",
              "...                                                   ...\n",
              "321691  স্বাস্থ্যকর জীবনযাপনের জন্য সুষম খাবার জরুরি। ...\n",
              "321692  শীতে সর্দি-কাশি সমস্যা হয়ে থাকে। অনেক সময় দেখা...\n",
              "321693  বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়ে কর্মকর্তা নিয়...\n",
              "321694  অনেকের কোথাও ঘুরতে যাওয়ার সময়, দীর্ঘক্ষণ বাসে-...\n",
              "321695  শীত এলেই বেড়ে যায় খুশকির সমস্যা। চুল ঝরা, রুক্...\n",
              "\n",
              "[321696 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OONhDP5vAgM",
        "outputId": "76a76ca1-6d95-4f20-c154-b7f386ec1e25"
      },
      "source": [
        "'''#drop Summary Field\n",
        "articles.drop('Summary', inplace=True, axis=1)\n",
        "articles.reset_index(drop=True)\n",
        "articles'''\n",
        "articles.dropna(inplace=True)\n",
        "articles.reset_index(drop=True)\n",
        "articles.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Full_Text    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWbJ2VNGvDgu",
        "outputId": "d92f1565-55b1-43d6-ee6d-b80b0b9b5231"
      },
      "source": [
        "random.seed(42)\n",
        "bangla_news_list = articles.Full_Text.to_list()\n",
        "print(bangla_news_list[31])\n",
        "random.shuffle(bangla_news_list)\n",
        "print(bangla_news_list[31])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " এ কাজে সিএসসিকে সহযোগিতা করবে বুয়েটের বিআরটিসি (ব্যুরো অফ রিসার্চ টেস্টিং অ্যান্ড কনসালটেন্ট)। রোববার এ বিষয়ে সিএসসির সঙ্গে রেলপথ মন্ত্রণালয়ের একটি চুক্তি হয়। রেলভবনে পদ্মা সেতু রেল সংযোগ প্রকল্প পরিচালক সুকুমার ভৌমিক, সেনাবাহিনী ও বিআরটিসির পক্ষ থেকে কর্নেল মো. আবুল কালাম আজাদ (প্রজেক্ট ম্যানজার, সিএসসি, পদ্মা সেতু রেল সংযোগ প্রকল্প) চুক্তিতে স্বাক্ষর করেন। পদ্মা সেতু রেল সংযোগ প্রকল্পের জন্য সিএসসিকে সুপারভিশন পরামর্শক হিসেবে নিয়োগের বিষয়টি গত ২০ ডিসেম্বর সরকারি ক্রয় সংক্রান্ত মন্ত্রিসভা কমিটির সায় পায়। এ কাজে ব্যয় ধরা হয়েছে ৯৪১ কোটি টাকা। পরামর্শক সেবাকাল দুই হাজার ১০০ দিন। এই সেবার আওতায় সেনাবাহিনীর ইঞ্জিনিয়ারিং কোরের সদস্যরা ঠিকাদারের দাখিল করা নকশা যাচাই করে তার অনুমোদন দেবেন। এছাড়া রেলের জন্য ভূমি অধিগ্রহণ, সাইট ক্লিয়ারেন্স এবং পূর্নবাসন কার্যক্রমে সহায়তা ও তদারকি করবে। দেশি-বিদেশি বিশেশজ্ঞের সমন্বয়ে নির্মাণকাজ সুষ্ঠু তদারকিসহ পরিবেশের ভারসাম্য রক্ষা, আধুনিক প্রযুক্তির ব্যবহার এবং এ সংক্রান্ত প্রয়োজনীয় প্রশিক্ষণসহ যোগ্যতা বৃদ্ধি করা পরামর্শক সেবার মধ্যে রয়েছে। চুক্তিস্বাক্ষর অনুষ্ঠানে রেলপথ মন্ত্রী মুজিবুল হক বলেন, “পদ্মা সেতুতে একই সাথে রেল চলাচলের জন্য উদ্যোগ নেওয়া হয়েছে। সঠিক সময় নির্মাণকাজ শেষ করতে পরামর্শক কাজ সেনাবাহিনীকে দেওয়া হয়েছে।” অনুষ্ঠানে সেনা প্রধান জেনারেল আবু বেলাল মোহাম্মদ শফিউল হক বলেন, “ইতিমধ্যে সেনাবাহিনীর কাছে ন্যস্ত পদ্মা বহুমুখী সেতুর ৩টি প্রকল্পের মধ্যে দুটি প্রকল্প যথাসময়ে সম্পন্ন করে বাংলাদেশ ব্রিজ অথরিটির কাছে হস্তান্তর করা হয়েছে। এর মাধ্যমে সিএসসির কর্মদক্ষতা ও পেশাদারিত্বের নতুন মাইল ফলক রচিত হয়েছে।” বহু প্রতীক্ষিত পদ্মা সেতুর মূল কাঠামোর নির্মাণ কাজ চলছে এখন। ২০১৮ সাল নাগাদ এই সেতু চলাচলের জন্য খুলে দেওয়া যাবে এবং ওই সময় পদ্মাসেতুতে ট্রেন চলাচলও শুরু করা সম্ভব হবে বলে আশা করছে সরকার। পদ্মা সেতু রেল সংযোগ প্রকল্পের আওতায় ঢাকা থেকে যশোর পর্যন্ত ১৭২ কিলোমিটার ব্রডগেজ রেলপথ নির্মাণে গত অগাস্টে চীনের একটি প্রতিষ্ঠানের সঙ্গে চুক্তি করে রেলপথ মন্ত্রণালয়। এ প্রকল্পের আওতায় ১৪টি নতুন স্টেশন ভবন ও ছয়টি স্টেশন পুনর্নির্মাণ, ৬৬টি গুরুত্বপূর্ণ সেতু এবং ২৪৪টি ছোট সেতু নির্মাণ, ১০০টি নতুন রেলকোচ কেনা এবং আধুনিক সিগন্যাল সিস্টেমের জন্য অপটিক্যাল ফাইবার লাইন স্থাপন করা হবে।\n",
            "নীলফামারীতে চলন্ত ভ্যানে শিশু জন্ম দিলেন রুবিনা আকতার (৩০) এক গৃহবধূ। প্রসূতির সঙ্গে থাকা লোকজনের অগোচরে জন্ম নিয়েই সড়কে ছিটকে পড়ে মৃত্যুও হয়েছে ওই নবজাতকের। আজ মঙ্গলবার দুপুর আড়াইটার দিকে জেলা শহরের পাঁচমাথা মোড় এলাকায় এ ঘটনা ঘটে।রুবিনা জেলা সদরের চড়াইখোলা ইউনিয়নের যাদুরহাট বাড়াইপাড়া গ্রামের দিনমজুর মহুবার রহমানের স্ত্রী। তাকে নীলফামারী জেনারেল হাসপাতালে ভর্তি করা হয়েছে। গাইনী ওয়ার্ডে চিকিৎসাধীন আছেন।প্রত্যক্ষ্যদর্শী কয়েকজন ব্যাক্তি জানান, দুপুর আড়াটার দিকে গর্ভবতী এক নারীকে ওই সড়ক দিয়ে ভ্যানে নীলফামারী জেনারেল হাসপাতালে নেওয়া হচ্ছিল। এ সময় হঠাৎ করে ওই একটি নবজাতক পাকা সড়কের ওপর ছিটকে পড়ে। প্রত্যক্ষদর্শীদের চিৎকারে ভ্যান থামিয়ে নবজাতকের স্বজনরা শিশুটিকে তুলে নিয়ে হাসপাতালে যায়।রুবিনা আকতারের জা (দেবরের স্ত্রী) ফাতেমা বেগম (২৫) বলেন, মঙ্গলবার বেলা ১২টার দিকে রুবিনার প্রসব ব্যাথা ওঠে। এ অবস্থায় গ্রামে ভ্যান পেতে কিছুটা সময় পেড়িয়ে যায়। দুপুর ২টার দিকে একখানা অটোভ্যান (ব্যাটারি চালিত) নিয়ে তাকে হাসপাতালে নেওয়ার জন্য রওয়ানা দেই। রুবিনা আকতার আমার কোলে মাথা দেয়। পেছনে রুবিনার এক বড় বোন ছিল। শহরের পাঁচ মাথা অতিক্রম করার সময় স্থানীয় লোকজন চিৎকার করে জানায় বাচ্চা (নবজাতক) পড়ে গেছে বলে। আমি ভ্যান থেকে নেমে বাচ্চাটিকে কোলে নিয়ে দ্রুত হাসপাতালে যাই। এরপর কর্তব্যরত চিকিৎসক বাচ্চাটিকে দেখে মৃত ঘোষণা করেন।হাসপাতালের জরুরি বিভাগের কর্তব্যরত চিকিৎসক মো. নুরুল হুদা বলেন, বেলা ৩টার দিকে প্রসূতি মা ও নবজাতককে নিয়ে আসেন তার স্বজনরা। শিশুটি আঘাত জনিত কারণে হাসপাতালে আনার আগে মারা গেছে। তার মা রুবিনা আকতারের রক্তক্ষরণ হচ্ছে। তাই হাসপাতালে ভর্তি করে তার চিকিৎসা দেওয়া হচ্ছে।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RurYs6MuQaj"
      },
      "source": [
        "# Pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6V7Mps6uBxm"
      },
      "source": [
        "stopwords_2 = \"অবশ্য অনেক অনেকে অনেকেই অন্তত অথবা অথচ অর্থাত অন্য আজ আছে আপনার আপনি আবার আমরা আমাকে আমাদের আমার আমি আরও আর আগে আগেই আই অতএব আগামী অবধি অনুযায়ী আদ্যভাগে এই একই একে একটি এখন এখনও এখানে এখানেই এটি এটা এটাই এতটাই এবং একবার এবার এদের এঁদের এমন এমনকী এল এর এরা এঁরা এস এত এতে এসে একে এক এ ঐ ই ইহা ইত্যাদি উনি উপর উপরে উচিত ও ওই ওর ওরা ওঁর ওঁরা ওকে ওদের ওঁদের ওখানে কত কবে করতে কয়েক কয়েকটি করবে করলেন করার কারও করা করি করিয়ে করার করাই করলে করলেন করিতে করিয়া করেছিলেন করছে করছেন করেছেন করেছে করেন করবেন করায় করে করেই কাছ কাছে কাজে কারণ কিছু কিছুই কিন্তু কিংবা কি কী কেউ কেউই কাউকে কেন কে কোনও কোনো কোন কখনও ক্ষেত্রে খুব গুলি গিয়ে গিয়েছে গেছে গেল গেলে গোটা চলে ছাড়া ছাড়াও ছিলেন ছিল জন্য জানা ঠিক তিনি তিনঐ তিনিও তখন তবে তবু তাঁদের তাঁাহারা তাঁরা তাঁর তাঁকে তাই তেমন তাকে তাহা তাহাতে তাহার তাদের তারপর তারা তারৈ তার তাহলে তিনি তা তাও তাতে তো তত তুমি তোমার তথা থাকে থাকা থাকায় থেকে থেকেও থাকবে থাকেন থাকবেন থেকেই দিকে দিতে দিয়ে দিয়েছে দিয়েছেন দিলেন দু দুটি দুটো দেয় দেওয়া দেওয়ার দেখা দেখে দেখতে দ্বারা ধরে ধরা নয় নানা না নাকি নাগাদ নিতে নিজে নিজেই নিজের নিজেদের নিয়ে নেওয়া নেওয়ার নেই নাই পক্ষে পর্যন্ত পাওয়া পারেন পারি পারে পরে পরেই পরেও পর পেয়ে প্রতি প্রভৃতি প্রায় ফের ফলে ফিরে ব্যবহার বলতে বললেন বলেছেন বলল বলা বলেন বলে বহু বসে বার বা বিনা বরং বদলে বাদে বার বিশেষ বিভিন্ন বিষয়টি ব্যবহার ব্যাপারে ভাবে ভাবেই মধ্যে মধ্যেই মধ্যেও মধ্যভাগে মাধ্যমে মাত্র মতো মতোই মোটেই যখন যদি যদিও যাবে যায় যাকে যাওয়া যাওয়ার যত যতটা যা যার যারা যাঁর যাঁরা যাদের যান যাচ্ছে যেতে যাতে যেন যেমন যেখানে যিনি যে রেখে রাখা রয়েছে রকম শুধু সঙ্গে সঙ্গেও সমস্ত সব সবার সহ সুতরাং সহিত সেই সেটা সেটি সেটাই সেটাও সম্প্রতি সেখান সেখানে সে স্পষ্ট স্বয়ং হইতে হইবে হৈলে হইয়া হচ্ছে হত হতে হতেই হবে হবেন হয়েছিল হয়েছে হয়েছেন হয়ে হয়নি হয় হয়েই হয়তো হল হলে হলেই হলেও হলো হিসাবে হওয়া হওয়ার হওয়ায় হন হোক জন জনকে জনের জানতে জানায় জানিয়ে জানানো জানিয়েছে জন্য জন্যওজে জে বেশ দেন তুলে ছিলেন চান চায় চেয়ে মোট যথেষ্ট টি অতএব অথচ অথবা অনুযায়ী অনেক অনেকে অনেকেই অন্তত অন্য অবধি অবশ্য অর্থাত আই আগামী আগে আগেই আছে আজ আদ্যভাগে আপনার আপনি আবার আমরা আমাকে আমাদের আমার আমি আর আরও ই ইত্যাদি ইহা উচিত উত্তর উনি উপর উপরে এ এঁদের এঁরা এই একই একটি একবার একে এক্ এখন এখনও এখানে এখানেই এটা এটাই এটি এত এতটাই এতে এদের এব এবং এবার এমন এমনকী এমনি এর এরা এল এস এসে ঐ ও ওঁদের ওঁর ওঁরা ওই ওকে ওখানে ওদের ওর ওরা কখনও কত কবে কমনে কয়েক কয়েকটি করছে করছেন করতে করবে করবেন করলে করলেন করা করাই করায় করার করি করিতে করিয়া করিয়ে থাকবেন থাকা থাকায় থাকে থাকেন থেকে থেকেই থেকেও দিকে দিতে দিন দিয়ে দিয়েছে দিয়েছেন দিলেন দু দুই দুটি দুটো দেওয়া দেওয়ার দেওয়া দেখতে দেখা দেখে দেন দেয় দ্বারা ধরা ধরে ধামার নতুন নয় না নাই নাকি নাগাদ নানা নিজে নিজেই নিজেদের নিজের নিতে নিয়ে নিয়ে নেই নেওয়া নেওয়ার নেওয়া নয় পক্ষে পর পরে পরেই পরেও পর্যন্ত পাওয়া পাচ পারি পারে পারেন পি পেয়ে পেয়্র্ প্রতি প্রথম প্রভৃতি প্রযন্ত প্রাথমিক প্রায় প্রায় ফলে ফিরে ফের বক্তব্য বদলে বন বরং বলতে বলল বললেন বলা বলে বলেছেন বলেন বসে হু বা বাদে বার বি বিনা বিভিন্ন বিশেষ বিষয়টি  বেশ বেশি ব্যবহার ব্যাপারে ভাবে ভাবেই মতো মতোই মধ্যভাগে মধ্যে মধ্যেই মধ্যেও মনে মাত্র মাধ্যমে মোট মোটেই যখন যত যতটা যথেষ্ট যদি যদিও যা যাঁর যাঁরা যাওয়া যাওয়ার যাওয়া যাকে যাচ্ছে যাতে যাদের যান যাবে যায় যার যারা যিনি যে যেখানে যেতে যেন যেমন র রকম রয়েছে রাখা রেখে লক্ষ শুধু শুরু সঙ্গে সঙ্গেও সব সবার সমস্ত সম্প্রতি সহ সহিত সাধারণ সামনে সি সুতরাং সে সেই নয় না হয় হয়ে হয়েছে হয়েছিলেন । , - ; ঃ মিথ্যা শুরু\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIekHSFluHkN",
        "outputId": "f225d543-be98-410c-a27f-db8a42e030db"
      },
      "source": [
        "stopwords = []\n",
        "\n",
        "for word in stopwords_1:\n",
        "  if word not in stopwords:\n",
        "    stopwords.append(word)\n",
        "\n",
        "for word in stopwords_2.split():\n",
        "  if word not in stopwords:\n",
        "    stopwords.append(word)\n",
        "\n",
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6KAJr5txwdF"
      },
      "source": [
        "def preprocess_BN(text, stop_words=stopwords):\n",
        "  #text = text.split()\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'\\<a href', ' ', text)\n",
        "  text = re.sub(r'&amp;', '', text) \n",
        "  text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "  text = re.sub(r'<br />', ' ', text)\n",
        "  text = re.sub(r'\\'', ' ', text)\n",
        "  text = re.sub(r'।', ' ', text)\n",
        "  text = re.sub(r'[‘’]', ' ', text)\n",
        "  text = re.sub(r'[০১২৩৪৫৬৭৮৯123456789]', ' ', text)\n",
        "  #text = \" \".join(text)\n",
        "  \n",
        "  tokenizer = Tokenizers()\n",
        "  stemmer = RafiStemmer()\n",
        "\n",
        "  #stop_words = load_stop_word()\n",
        "  \n",
        "\n",
        "  doc_token = []\n",
        "\n",
        "  if isinstance(text, str):\n",
        "    for token in tokenizer.bn_word_tokenizer(text):\n",
        "      if token not in stop_words and len(token) >= 3:\n",
        "        stemmed_token = stemmer.stem_word(token)\n",
        "        if len(stemmed_token) >= 2 and stemmed_token not in stop_words:\n",
        "          doc_token.append(stemmed_token)\n",
        "\n",
        "\n",
        "  #text = remove_stopwords(text, stopwords=stopwords)\n",
        "\n",
        "  '''if isinstance(text, str):\n",
        "    text = text.split()\n",
        "  text = stemming_docs(text)'''\n",
        "  #print(text)\n",
        "  return \" \".join(doc_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK0f9aLex3hl"
      },
      "source": [
        "# remove symbols, punctuations, and stopwords\n",
        "\n",
        "pp_bangla_news_list = []\n",
        "for bangla_news in bangla_news_list:\n",
        "  #print(\"Type: {}\".format(type(str(bangla_news))))\n",
        "  #print(\"Before PP: {}\".format(bangla_news))\n",
        "  pp_bangla_news_list.append(preprocess_BN(bangla_news))\n",
        "  #print(\"After PP: {}\\n\\n\".format(pp_bangla_news_list[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RikveP7YSkYk"
      },
      "source": [
        "articles = pd.read_csv('/content/drive/MyDrive/THESIS/Final Dataset/without Category/plsa_stopwords_removed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nKAa7q4RMqf"
      },
      "source": [
        "import regex \n",
        "\n",
        "def custom_analyzer(text):\n",
        "    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
        "    for w in words:\n",
        "        yield w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwPih5O4yy49"
      },
      "source": [
        "# Vectorize the dataset FOR BANGLA\n",
        "vectorizer_BN = CountVectorizer(analyzer=custom_analyzer,\n",
        "                                max_df=0.8, min_df=0.002) #'''max_df=0.5, min_df=2'''\n",
        "vectorizer_BN.fit_transform(pp_bangla_news_list[:150000])\n",
        "#docs_BN = torch.from_numpy(vectorizer_BN.fit_transform(pp_bangla_news_list[:75000]).toarray())\n",
        "#print(\"Changes Accepted.\")\n",
        "'''vocab_BN = pd.DataFrame(columns=['word', 'index'])\n",
        "vocab_BN['word'] = vectorizer_BN.get_feature_names()\n",
        "vocab_BN['index'] = vocab_BN.index'''\n",
        "\n",
        "vocab = vectorizer_BN.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMZ7497gUp57"
      },
      "source": [
        "final_docs = []\n",
        "\n",
        "for doc in pp_bangla_news_list[:150000]:\n",
        "    doc_token = []\n",
        "\n",
        "    for token in doc.split():\n",
        "        if token in vocab:\n",
        "            doc_token.append(token)\n",
        "    final_docs.append(\" \".join(doc_token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87xRbMh0a8Hq",
        "outputId": "ddaa9e0a-cdf9-4b35-8813-f1d80951ed94"
      },
      "source": [
        "print(len(pp_bangla_news_list[534]))\n",
        "print(len(final_docs[534]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2187\n",
            "1738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhsvRMcHkzbT"
      },
      "source": [
        "# pLSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnKX-fm-w-I2"
      },
      "source": [
        "!pip install git+https://github.com/banglakit/bengali-stemmer.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm4xzUCnl2v0"
      },
      "source": [
        "#import docx\n",
        "from bengali_stemmer.rafikamal2014 import RafiStemmer\n",
        "#from bnltk.stemmer import BanglaStemmer\n",
        "import string\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWJFwsxErSDF"
      },
      "source": [
        "def read_doc_as_pandasDF(filename):\n",
        "\n",
        "    data = pd.read_csv(filename, error_bad_lines=False)\n",
        "    data_text = data[['Full_Text']]\n",
        "\n",
        "    data_text['index'] = data_text.index\n",
        "    documents = data_text\n",
        "\n",
        "    return (documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdcfaairrXIC"
      },
      "source": [
        "def punctuation_remover(text):\n",
        "    BENGALI_PUNCTUATION = string.punctuation + \"—।’‘\"\n",
        "    BENGALI_NUMERALS = \"০১২৩৪৫৬৭৮৯\"\n",
        "    return text.translate(str.maketrans(' ', ' ', BENGALI_PUNCTUATION+BENGALI_NUMERALS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGVOvWeqrmHE"
      },
      "source": [
        "def load_stop_word(stops_dir = '/content/drive/My Drive/THESIS/bn_stopwords.txt'):\n",
        "\n",
        "    '''stop_directory = stops_dir\n",
        "\n",
        "    doc = docx.Document(stop_directory)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)'''\n",
        "\n",
        "    bengali_stop_words = []\n",
        "    with open(stops_dir, 'r') as f:\n",
        "      for word in f.readlines():\n",
        "        bengali_stop_words.append(word)\n",
        "        \n",
        "    bengali_stop_words = frozenset(bengali_stop_words)\n",
        "\n",
        "    return bengali_stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHg7NkJlr1lG"
      },
      "source": [
        "def preprocess_documents(text):\n",
        "\n",
        "    '''preprocessed_list_of_docs = []\n",
        "\n",
        "    tokenizer = Tokenizers()\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "    preprocessed_docs = []\n",
        "\n",
        "    doc_token = []\n",
        "\n",
        "    if isinstance(doc, str):\n",
        "        for token in tokenizer.bn_word_tokenizer(doc):\n",
        "            if token not in stop_words and len(token) >= 3:\n",
        "                stemmed_token = stemmer.stem_word(token)\n",
        "                if len(stemmed_token) >= 2 and stemmed_token not in stop_words:\n",
        "                    doc_token.append(stemmed_token)\n",
        "\n",
        "\n",
        "    return doc_token'''\n",
        "\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r'।', ' ', text)\n",
        "    text = re.sub(r'[‘’]', ' ', text)\n",
        "    text = re.sub(r'[০১২৩৪৫৬৭৮৯123456789]', ' ', text)\n",
        "    text = re.sub(r'[a-zA-Z]', ' ', text)\n",
        "    #text = \" \".join(text)\n",
        "  \n",
        "    #print(\"In PP (before stopwords): {}\".format(text))\n",
        "    '''text = text.split(' ')\n",
        "    text = [word for word in text if word not in stopwords]'''\n",
        "  \n",
        "    tokenizer = Tokenizers()\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "\n",
        "    doc_token = []\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        for token in tokenizer.bn_word_tokenizer(text):\n",
        "            if token not in stop_words and len(token) >= 3:\n",
        "                stemmed_token = stemmer.stem_word(token)\n",
        "                if len(stemmed_token) >= 2 and stemmed_token not in stop_words:\n",
        "                    doc_token.append(stemmed_token)\n",
        "\n",
        "\n",
        "    #text = remove_stopwords(text, stopwords=stopwords)\n",
        "\n",
        "    '''if isinstance(text, str):\n",
        "        text = text.split()\n",
        "    text = stemming_docs(text)'''\n",
        "    #print(text)\n",
        "    return doc_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhcZGdCr6Ey"
      },
      "source": [
        "def prepare_bag_of_words(processed_docs, dictionary):\n",
        "    return [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UXNNSWYr_kq"
      },
      "source": [
        "def prepare_bow_list(bow_corpus, dictionary):\n",
        "\n",
        "    header_list = list(range(0, len(dictionary)-1))\n",
        "    all_list = [header_list]\n",
        "\n",
        "    for each_list in bow_corpus:\n",
        "        temp_list = [0]*len(dictionary)\n",
        "        for each_tuple in each_list:\n",
        "            temp_list[each_tuple[0]] = each_tuple[1]\n",
        "        all_list.append(temp_list)\n",
        "\n",
        "    minimal_all_list = []\n",
        "\n",
        "    minimal_header_list = []\n",
        "\n",
        "    for i in range(len(dictionary)):\n",
        "        minimal_header_list.append(dictionary[i])\n",
        "\n",
        "    minimal_all_list.append(minimal_header_list)\n",
        "\n",
        "    for each_mini_list in all_list[1:]:\n",
        "        minimal_all_list.append(each_mini_list)\n",
        "\n",
        "\n",
        "    return (minimal_all_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guo7VDGpsIfo"
      },
      "source": [
        "def prepare_model_list_presentation(lda_model, dictionary):\n",
        "\n",
        "    header_list =  [lda_model.id2word[i] for i in range(0, len(dictionary))]\n",
        "    list_for_csv = [header_list]\n",
        "\n",
        "    for i in range(lda_model.num_topics):\n",
        "        each_topic_list = lda_model.get_topic_terms(i, topn=len(dictionary))\n",
        "        sorted_each_topic_list = sorted(each_topic_list, key=lambda k: k[0])\n",
        "        just_value_list = [each_tuple[1] for each_tuple in sorted_each_topic_list]\n",
        "\n",
        "        list_for_csv.append(just_value_list)\n",
        "\n",
        "    return (list_for_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w3d3vxUsSDA"
      },
      "source": [
        "def prepare_plsa_model(bow_corpus, n_topics):\n",
        "    plsa_model_normal = plsa.algorithms.plsa.PLSA(bow_corpus, n_topics=n_topics, tf_idf=False)\n",
        "    return plsa_model_normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "1xkY4vKNxuUo",
        "outputId": "81e08dc0-85c1-466b-a0ed-e1bee3ca9eda"
      },
      "source": [
        "np.random.seed(2021)\n",
        "\n",
        "NUM_TOPICS = 20\n",
        "CSV_LOCATION = '/content/drive/MyDrive/THESIS/Final Dataset/all_final_randomized.csv'\n",
        "    \n",
        "pd_document = read_doc_as_pandasDF(CSV_LOCATION)\n",
        "\n",
        "smaller_documents = pd_document[:100000]\n",
        "processed_docs = smaller_documents['Full_Text'].map(preprocess_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-91a43e9ea982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m smaller_documents = pd_document[:100000]'''\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprocessed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmaller_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Full_Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'smaller_documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzTO_avMk4gk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "27ce899c-b2fa-4b10-b6a2-83417c3aa07b"
      },
      "source": [
        "'''np.random.seed(2021)\n",
        "\n",
        "NUM_TOPICS = 20\n",
        "CSV_LOCATION = '/content/drive/MyDrive/THESIS/Final Dataset/all_final_randomized.csv'\n",
        "    \n",
        "pd_document = read_doc_as_pandasDF(CSV_LOCATION)\n",
        "\n",
        "smaller_documents = pd_document.Full_Text[:100000]\n",
        "processed_docs = smaller_documents[['Full_Text']].map(preprocess_documents)'''\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "dictionary.filter_extremes(no_below=.01, no_above=0.7)\n",
        "#, keep_n=100000\n",
        "\n",
        "bow_corpus = prepare_bag_of_words(processed_docs, dictionary)\n",
        "plsa_model_normal = prepare_plsa_model(bow_corpus, n_topics=NUM_TOPICS)\n",
        "\n",
        "plsa_model_result = plsa_model_normal.fit(eps=1e-05, max_iter=50, warmup=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4b020ae99da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbow_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplsa_model_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_plsa_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TOPICS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplsa_model_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplsa_model_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-20fd17bfa7f2>\u001b[0m in \u001b[0;36mprepare_plsa_model\u001b[0;34m(bow_corpus, n_topics)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_plsa_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplsa_model_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPLSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplsa_model_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/plsa/algorithms/plsa.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, n_topics, tf_idf)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_given_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/plsa/algorithms/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, n_topics, tf_idf)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__n_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__n_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/plsa/algorithms/base.py\u001b[0m in \u001b[0;36m__validated\u001b[0;34m(corpus, n_topics)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There must be at least 2 topics!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_docs\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             msg = (f'The number of both, documents (= {corpus.n_docs}) '\n\u001b[1;32m    208\u001b[0m                    \u001b[0;34mf'and words (= {corpus.n_words}), must be greater than'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'n_docs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eKSf6D7LQ2H",
        "outputId": "35108928-37e1-4ed4-c0b6-63fdc8d55879"
      },
      "source": [
        "#bn_stopwords = []\n",
        "with open('/content/drive/MyDrive/THESIS/bn_stopwords.txt', 'r') as fp:\n",
        "    stopwords = [word.split()[0] for word in fp.readlines()]\n",
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSAvz9_UjzA6"
      },
      "source": [
        "def remove_symbols(text):\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r'।', ' ', text)\n",
        "    text = re.sub(r'[‘’]', ' ', text)\n",
        "    text = re.sub(r'[০১২৩৪৫৬৭৮৯123456789]', ' ', text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ouODnHdjERl"
      },
      "source": [
        "def remove_stopwords(text, stop_words=stopwords):\n",
        "  #text = text.split()\n",
        "  tokenizer = Tokenizers()\n",
        "  stemmer = RafiStemmer()\n",
        "\n",
        "  #stop_words = load_stop_word()\n",
        "  \n",
        "\n",
        "  doc_token = []\n",
        "\n",
        "  if isinstance(text, str):\n",
        "    for token in tokenizer.bn_word_tokenizer(text):\n",
        "      if token not in stop_words and len(token) >= 3:\n",
        "        stemmed_token = stemmer.stem_word(token)\n",
        "        if len(stemmed_token) >= 2 and stemmed_token not in stop_words:\n",
        "          doc_token.append(stemmed_token)\n",
        "\n",
        "\n",
        "  #text = remove_stopwords(text, stopwords=stopwords)\n",
        "\n",
        "  '''if isinstance(text, str):\n",
        "    text = text.split()\n",
        "  text = stemming_docs(text)'''\n",
        "  #print(text)\n",
        "  return \" \".join(doc_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb3JEB4rFX_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24cce600-1162-4390-fcad-bda9f1bdd679"
      },
      "source": [
        "np.random.seed(2021)\n",
        "\n",
        "NUM_TOPICS = 20\n",
        "CSV_LOCATION = '/content/drive/MyDrive/THESIS/Final Dataset/all_final_reduced.csv'\n",
        "    \n",
        "pd_document = read_doc_as_pandasDF(CSV_LOCATION)\n",
        "\n",
        "smaller_documents = pd_document.Full_Text.to_list()[:100000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAVg4HelbX-2",
        "outputId": "22396fc7-7345-47be-e29b-b1905843db6f"
      },
      "source": [
        "import sys\n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(850000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "850000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17FdgibT-RIs"
      },
      "source": [
        "from plsa.preprocessors import *\n",
        "tokenizer = Tokenizers()\n",
        "CUSTOM_PIPELINE = (\n",
        "    remove_numbers,\n",
        "    remove_tags('<[^>]*>'),\n",
        "    remove_punctuation(string.punctuation),\n",
        "    tokenizer.bn_word_tokenizer ,\n",
        "    RemoveStopwords(stopwords),\n",
        "    remove_short_words(3)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLWNyXWHBqvp"
      },
      "source": [
        "NUM_TOPICS = 20\n",
        "CSV_LOCATION = '/content/drive/MyDrive/THESIS/Final Dataset/without Category/plsa_final_dataset.csv'\n",
        "#pipes = (remove_symbols, remove_stopwords)\n",
        "pipeline = plsa.pipeline.Pipeline(*CUSTOM_PIPELINE)\n",
        "bow_corpus = plsa.corpus.Corpus.from_csv(CSV_LOCATION, pipeline, encoding='utf-8', max_docs=150000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQOHK8cHDc9n"
      },
      "source": [
        "bow_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkEK5UeKA8gi"
      },
      "source": [
        "plsa_model_normal = prepare_plsa_model(bow_corpus, n_topics=NUM_TOPICS)\n",
        "\n",
        "plsa_model_result = plsa_model_normal.fit(eps=1e-05, max_iter=50, warmup=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eokaZidS6QW"
      },
      "source": [
        "topics_prod = []\n",
        "for i in range(20):\n",
        "    ls = [tup[0] for tup in plsa_model_result.word_given_topic[i][:10]]\n",
        "    topics_prod.append(ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9fg8Rv2OFnP",
        "outputId": "bc6c2cfc-75fa-49dc-c8a9-10d7e36a5eaf"
      },
      "source": [
        "topics_prod"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['পুলিশ',\n",
              "  'গ্রাম',\n",
              "  'ঘটনা',\n",
              "  'ছেল',\n",
              "  'জানান',\n",
              "  'উপজেল',\n",
              "  'হাসপাতাল',\n",
              "  'থান',\n",
              "  'উদ্ধ',\n",
              "  'লাশ'],\n",
              " ['রান',\n",
              "  'ম্যাচ',\n",
              "  'বাংলাদেশ',\n",
              "  'উইকেট',\n",
              "  'গোল',\n",
              "  'শেষ',\n",
              "  'তিন',\n",
              "  'মিনিট',\n",
              "  'ইনিংস',\n",
              "  'ওভার'],\n",
              " ['শেখ',\n",
              "  'প্রধানমন্ত্রী',\n",
              "  'বাংলাদেশ',\n",
              "  'দেশ',\n",
              "  'হাসিনা',\n",
              "  'কথা',\n",
              "  'সাল',\n",
              "  'মানুষ',\n",
              "  'বঙ্গবন্ধু',\n",
              "  'জাতী'],\n",
              " ['মামলা',\n",
              "  'আদালত',\n",
              "  'সাল',\n",
              "  'আবেদন',\n",
              "  'শ্রমিক',\n",
              "  'বছর',\n",
              "  'আদাল',\n",
              "  'কোর্ট',\n",
              "  'আইন',\n",
              "  'মামল'],\n",
              " ['করোনাভাইরাস',\n",
              "  'আক্রান্ত',\n",
              "  'মৃত্যু',\n",
              "  'দেশ',\n",
              "  'করোনা',\n",
              "  'রোগী',\n",
              "  'শনাক্ত',\n",
              "  'হাসপাতাল',\n",
              "  'সংখ্যা',\n",
              "  'লাখ'],\n",
              " ['সম্পাদক',\n",
              "  'সভাপতি',\n",
              "  'কমি',\n",
              "  'সদস্য',\n",
              "  'চৌধুরী',\n",
              "  'রহমান',\n",
              "  'নেতা',\n",
              "  'হোস',\n",
              "  'জাতী',\n",
              "  'জেলা'],\n",
              " ['গান',\n",
              "  'শুভ',\n",
              "  'ভালো',\n",
              "  'জীবন',\n",
              "  'চলচ্চিত্র',\n",
              "  'অভিন',\n",
              "  'নাটক',\n",
              "  'নাম',\n",
              "  'আপনা',\n",
              "  'করুন'],\n",
              " ['যুক্তরাষ্ট্র',\n",
              "  'প্রেসিডেন্ট',\n",
              "  'ট্রাম্প',\n",
              "  'মার্কিন',\n",
              "  'সাল',\n",
              "  'তুরস্ক',\n",
              "  'দেশ',\n",
              "  'কিম',\n",
              "  'অ্যাপ',\n",
              "  'বছর'],\n",
              " ['লীগ',\n",
              "  'আওয়ামী',\n",
              "  'নির্বাচন',\n",
              "  'প্রার্থী',\n",
              "  'ভোট',\n",
              "  'বিএনপি',\n",
              "  'নেতা',\n",
              "  'বিএনপির',\n",
              "  'সম্পাদক',\n",
              "  'ওয়ার্ড'],\n",
              " ['কথা',\n",
              "  'ভারত',\n",
              "  'মানুষ',\n",
              "  'ভালো',\n",
              "  'পাকিস্তান',\n",
              "  'বছর',\n",
              "  'বাংলাদেশ',\n",
              "  'দেশ',\n",
              "  'ক্রিকেট',\n",
              "  'সাল'],\n",
              " ['পুলিশ',\n",
              "  'অভিযান',\n",
              "  'অভিযোগ',\n",
              "  'গ্রেপ্ত',\n",
              "  'এলাকা',\n",
              "  'র\\u200c্যাব',\n",
              "  'আটক',\n",
              "  'নারী',\n",
              "  'সদস্য',\n",
              "  'ধর্ষণ'],\n",
              " ['ব্যাংক',\n",
              "  'প্রতিষ্ঠান',\n",
              "  'টাকা',\n",
              "  'বাংলাদেশ',\n",
              "  'বছর',\n",
              "  'শতাংশ',\n",
              "  'পয়সা',\n",
              "  'তথ্য',\n",
              "  'পরিচালক',\n",
              "  'টাক'],\n",
              " ['শতাংশ',\n",
              "  'টাকা',\n",
              "  'লাখ',\n",
              "  'দশমিক',\n",
              "  'পণ্য',\n",
              "  'শিল্প',\n",
              "  'দেশ',\n",
              "  'বাংলাদেশ',\n",
              "  'মাস',\n",
              "  'দাম'],\n",
              " ['উপজেলা',\n",
              "  'জানান',\n",
              "  'কলেজ',\n",
              "  'হাসপাতাল',\n",
              "  'উপজেল',\n",
              "  'সড়ক',\n",
              "  'মেডিক',\n",
              "  'ঢাকা',\n",
              "  'সদর',\n",
              "  'স্বাস্থ্য'],\n",
              " ['পরীক্ষা',\n",
              "  'বন্ধ',\n",
              "  'ঘটনা',\n",
              "  'মানুষ',\n",
              "  'তথ্য',\n",
              "  'ধরন',\n",
              "  'জানিয়',\n",
              "  'থাক',\n",
              "  'ফেসবুক',\n",
              "  'বিষ'],\n",
              " ['বাংলাদেশ',\n",
              "  'রোহিঙ্গা',\n",
              "  'ঢাকা',\n",
              "  'বাংলা',\n",
              "  'বিমান',\n",
              "  'ক্যাম্প',\n",
              "  'অনুষ্ঠান',\n",
              "  'লেখক',\n",
              "  'বিডিনিউজ',\n",
              "  'ডটকম'],\n",
              " ['নির্বাচন',\n",
              "  'শিশু',\n",
              "  'বিশ্ববিদ্যালয়',\n",
              "  'শিক্ষার্থী',\n",
              "  'শিক্ষক',\n",
              "  'বিশ্ববিদ্যাল',\n",
              "  'অধ্যাপক',\n",
              "  'কমিশন',\n",
              "  'কথা',\n",
              "  'নদী'],\n",
              " ['পানি',\n",
              "  'খাব',\n",
              "  'ভালো',\n",
              "  'শরীর',\n",
              "  'সমস্যা',\n",
              "  'রাখ',\n",
              "  'ত্বক',\n",
              "  'মানুষ',\n",
              "  'বন্যা',\n",
              "  'খাবার'],\n",
              " ['দেশ',\n",
              "  'চীন',\n",
              "  'বাংলাদেশ',\n",
              "  'ভারত',\n",
              "  'নাম',\n",
              "  'দক্ষিণ',\n",
              "  'রোহিঙ্গা',\n",
              "  'সম্মেলন',\n",
              "  'কোরিয়',\n",
              "  'প্রকাশ'],\n",
              " ['কৃষক',\n",
              "  'জেলা',\n",
              "  'ধান',\n",
              "  'চামচ',\n",
              "  'চামড়া',\n",
              "  'চাল',\n",
              "  'কৃষি',\n",
              "  'এলাকা',\n",
              "  'ব্যবসায়ী',\n",
              "  'জোন']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqSxc8MGQ5wG"
      },
      "source": [
        "final_articles_list = pd.read_csv('/content/drive/MyDrive/THESIS/Final Dataset/without Category/plsa_final_dataset.csv').Full_Text.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylVpd4qxXid_",
        "outputId": "c874bed0-4f6d-4cb9-c451-57a27d440b50"
      },
      "source": [
        "i = 0\n",
        "for doc in final_articles_list:\n",
        "    if i == 5:\n",
        "        break\n",
        "    i += 1\n",
        "    print(doc.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['তৃণমূল', 'যোগ', 'দক্ষিণ', 'বিধানসভা', 'কেন্দ্র', 'প্রার্থী', 'সোশ্যাল', 'মিডিয়', 'শিক', 'অভিনেত্রী', 'ঘোষ', 'সোশ্যাল', 'মিডিয়া', 'বিজেপি', 'আক্রমণ', 'ছাড়', 'ঘোষ', 'আগ', 'কথা', 'বিজেপির', 'মুখ', 'খুল', 'আক্রমণ', 'মুখ', 'পড়', 'ঘোষ', 'ঘটন', 'বিজেপি', 'নেতা', 'ঘোষ', 'টুইট', 'যুদ্ধ', 'শনিব', 'বিজেপি', 'খোলা', 'চিঠি', 'লিখ', 'চিঠির', 'কষ্ট', 'শান্তি', 'থাক', 'লম্বা', 'পোস্ট', 'লিখ', 'বিজেপি', 'আপনা', 'অ্যাটাক', 'ক্যাম্পেইন', 'চলচ্চিত্র', 'জগত', 'মানুষ', 'মানুষ', 'ব্যক্তি', 'সম্পর্', 'ভুল', 'তৈরি', 'আপনা', 'রাখ', 'বাংল', 'পুরনো', 'বাংল', 'মানুষ', 'ভালোবাসা', 'প্রতিষ্ঠিত', 'বোঝা', 'আপনা', 'একটু', 'অস্বস্তি', 'পড়', 'মানুষ', 'পরিষ্ক', 'দি', 'মানুষ', 'মেয়ে', 'সম্মান', 'আপনা', 'ধা', 'থাকব', 'আপনা', 'দল', 'নেতা', 'দেবী', 'প্রশ্ন', 'দল', 'নারী', 'সম্মান', 'কথা', 'আপনা', 'অস্বস্তি', 'পড়ব', 'এছাড়া', 'বাংল', 'মানুষ', 'মনোভাব', 'মুখ', 'ভাষা', 'আপনা', 'মেলা', 'আপনা', 'দল', 'নেতা', 'পরিবর্তন', 'স্লোগান', 'শেষ', 'লিখ', 'ঘোষ', 'দক্ষিণ', 'কেন্দ্র', 'তৃণমূল', 'প্রার্থী', 'ঘোষণা', 'মমতা', 'বন্দ্যোপাধ্যা', 'এদি', 'একাধিক', 'তারকা', 'প্রার্থী', 'তৃণমূল', 'ভিতর', 'চল', 'নেতৃত্ব', 'ক্ষোভ']\n",
            "['প্রতিষ্ঠ', 'সরকারি', 'অবকাঠামো', 'আবাসন', 'ব্যবস্থ', 'উন্নয়ন', 'রক্ষণাবেক্ষণ', 'অবকাঠামো', 'নির্মাণ', 'গুরুত্বপূর্ণ', 'গণপূর্ত', 'মন্ত্রণাল', 'আস', 'কোভিড', 'মহামারীর', 'চলমান', 'মন্ত্রণাল', 'অধীন', 'দপ্তর', 'দায়িত্ব', 'পালন', 'প্রকল্প', 'বাস্তবায়ন', 'গণপূর্ত', 'প্রকল্প', 'বাস্তবায়ন', 'গণপূর্ত', 'মন্ত্রণালয়', 'প্রকল্প', 'গতি', 'উল্লেখ', 'গণমাধ্যম', 'সংবাদ', 'পরিবেশন', 'প্রেক্ষি', 'বুধব', 'গণপূর্ত', 'মন্ত্রণালয়', 'সংবাদ', 'বিজ্ঞপ্তি', 'এসব', 'তথ্য', 'সংবাদ', 'বিজ্ঞপ্তি', 'সংশ্লিষ্ট', 'মন্ত্রণালয়', 'সরকার', 'অত্যন্ত', 'গুরুত্বপূর্ণ', 'মন্ত্রণালয়', 'বছর', 'মার্চ', 'মাস', 'করোনা', 'মহামারীর', 'সরক', 'সারাদেশ', 'লকডাউন', 'ঘোষণা', 'দেশ', 'সকল', 'কার্যক্রম', 'বাধাগ্রস্ত', 'গণপূর্ত', 'মন্ত্রণালয়', 'প্রকল্প', 'পিছি', 'পড়', 'তাছাড়া', 'ভূমি', 'অধিগ্রহণ', 'বিলম্ব', 'সাইট', 'পরিবর্তন', 'প্রকল্প', 'স্থান', 'বিদ্যমান', 'অবকাঠামো', 'নকশা', 'পরিবর্তন', 'সংক্রান্ত', 'কারন', 'প্রকল্প', 'বাস্তবায়ন', 'বাড়তি', 'সময়', 'প্রয়োজন', 'প্রকল্প', 'বাস্তবায়ন', 'বাড়তি', 'সম', 'লাগল', 'আসল', 'চিত্র', 'লকডাউন', 'মন্ত্রণাল', 'অধীন', 'জরুরী', 'স্বার্থ', 'লকডাউন', 'সম', 'সীমিত', 'পরিসর', 'অফ', 'বার্ষিক', 'উন্নয়ন', 'কর্মসূচির', 'আওতা', 'ডিসেম্বর', 'জাতী', 'অগ্রগতি', 'শতাংশ', 'গণপূর্ত', 'মন্ত্রণালয়', 'অগ্রগতি', 'শতাংশ', 'গতির', 'প্রশ্ন', 'ওঠ']\n",
            "['জীবন', 'লাগ', 'জিনিস', 'স্বাস্থ্যকর', 'বিপদ', 'অপচ', 'রোধ', 'তেল', 'নষ্ট', 'উপা', 'এসব', 'নি', 'আজক', 'আব', 'দূষণ', 'ঘট', 'দূষিত', 'উপাদান', 'দেহ', 'সুস্থ', 'কোষ', 'প্রবেশ', 'ক্রম', 'ধরন', 'স্বাস্থ্যগত', 'সমস্যা', 'এসব', 'দূষণ', 'উপাদান', 'সম', 'ক্যান্সার', 'ঝুঁকি', 'বাড়', 'দেহ', 'ক্ষতিকর', 'কোলেস্টেরল', 'মাত্', 'বৃদ্ধি', 'পা', 'সুষ্ঠু', 'বাধাগ্রস্ত', 'আরো', 'রোগ', 'তেল', 'বারব', 'রান্না', 'আরো', 'সমস্যা', 'হৃদরোগ', 'অসুবিধা', 'কড়া', 'দ্বিতীয়ব', 'ভালো', 'নির্ভর', 'ধরন', 'রান্না', 'তাপ', 'উত্তপ্ত', 'গুরুত্বপূর্ণ', 'ধরন', 'খাব', 'রান্না', 'হিসাব', 'বিষ', 'খেয়াল', 'চাইল', 'রান্না', 'শেষ', 'পরিমাণ', 'ফেল', 'অপচ', 'সম', 'একাধিকব', 'বিষ', 'মেন', 'চল', 'বেঁচ', 'ঠাণ্ডা', 'বাতাস', 'প্রবেশ', 'বোতল', 'সংরক্ষণ', 'যেসব', 'তেল', 'নষ্ট', 'পারব', 'খেয়াল', 'করুন', 'তুলনামূলক', 'ফেল', 'টাইমস', 'ইন্ডিয়া', 'অবলম্বন', 'সাকিব', 'সিকান্দ']\n",
            "['বছর', 'আরেক', 'হা', 'প্রোফাইল', 'ক্লাব', 'দায়িত্ব', 'গ্রহণ', 'ইচ্ছা', 'ব্যক্ত', 'চেলসির', 'দায়িত্ব', 'ইচ্ছা', 'বাস্তব', 'রূপ', 'সাল', 'রোমান', 'চেলসি', 'কিন', 'নেয়', 'কোচ', 'হিসেব', 'দায়িত্ব', 'যাচ্ছ', 'বছর', 'জার্মান', 'কোচ', 'গতকাল', 'বরখাস্ত', 'সাবেক', 'কোচ', 'দীর্ঘ', 'ইচ্ছা', 'প্রিমিয়', 'লিগ', 'ক্লাব', 'কোচ', 'দায়িত্ব', 'পালন', 'সাল', 'ছেড়', 'পরিবর্তিত', 'হিসেব', 'যোগ', 'চেলসির', 'আলোচনা', 'বস', 'করল', 'শেষ', 'প্যার', 'সেন্ট', 'পিএসজি', 'থিতু', 'বুধব', 'বিপক্ষ', 'প্রিমিয়', 'লিগ', 'ম্যাচ', 'চাপ', 'সুযোগ', 'থাকল', 'আপাতত', 'খেলোয়াড়', 'পর্যবেক্ষণ', 'চেলসি', 'বোর্ড', 'আশা', 'অপর', 'প্রতিদ্বন্দ্বী', 'কা', 'তুলনা', 'দল', 'ভাল', 'কারিগর', 'হিসেব', 'পারব', 'জার্মান', 'কোচ', 'বিবৃতি', 'সদ্য', 'সাবেক', 'কোচ', 'লিখ', 'চেলসি', 'পরিচালনা', 'ক্যারিয়ার', 'অন্যতম', 'সম্মান', 'বিষ', 'দীর্ঘ', 'সম', 'ক্লাব', 'জীবন', 'গুরুত্বপূর্ণ', 'অংশ', 'মৌসুম', 'ক্লাব', 'এগি', 'ব্যর্থ', 'হতাশ', 'ক্লাব', 'পক্ষ', 'বরখাস্ত', 'সহজ', 'সিদ্ধান্ত', 'খেলোয়াড়', 'হিসেব', 'চেলসির', 'বছর', 'ক্যারিয়ার', 'তিন', 'লিগ', 'লিগ', 'শিরোপা', 'চ্যাম্পিয়ন্স', 'লিগ', 'শিরোপা', 'জয়', 'কৃতিত্ব', 'দেখিয়', 'চেলসি', 'বিবৃতি', 'সাম্প্রতিক', 'ফলাফল', 'ক্লাব', 'প্রত্যাশ', 'সাথ', 'মানা', 'মৌসুম', 'ধরন', 'লক্ষ্য', 'কোচ', 'বরখাস্ত']\n",
            "['বিশ্ব', 'রেকর্ড', 'গড়', 'জীবন', 'ঝুঁকি', 'শক্ত', 'পরীক্ষ', 'মুখোমুখি', 'বিশ্ব', 'রেকর্ড', 'গড়', 'মৃত্যু', 'নেয়', 'প্রাণ', 'এভাব', 'বিশ্ব', 'রেকর্ড', 'নাম', 'তোলা', 'ভেব', 'দেখ', 'জানান', 'নদী', 'দিয়', 'রেকর্ড', 'তৈরি', 'নদী', 'ব্রাজিল', 'তৈরি', 'বিশ্ব', 'রেকর্ড', 'নদী', 'মানুষ', 'বিশ্ব', 'রেকর্ড', 'গড়', 'অদ্ভুত', 'কাণ্ড', 'ঘট', 'ব্রাজিল', 'ব্রাজিল', 'সেতু', 'একসঙ্গ', 'প্রতিযোগী', 'ঝুল', 'ইতোমধ্য', 'বুক', 'রেকর্ড', 'নাম', 'উঠ', 'পরিকল্পন', 'আগ', 'ঘট', 'ঘটনা', 'এপ্রিল', 'মাস', 'কর', 'সেব', 'অংশ', 'গ্রহণ', 'কর', 'বছর', 'সদস্য', 'সংখ্যা', 'রেকর্ড', 'স্থাপন', 'ফেল', 'ব্রাজিল', 'নাগরিক']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXGmUP8uNLGx",
        "outputId": "901cb074-50b4-4f87-e937-c5664ecb2303"
      },
      "source": [
        "new_doc = final_articles_list[52834:52835]\n",
        "\n",
        "topic_components, number_of_new_words, new_words = plsa_model_result.predict(new_doc[0])\n",
        "\n",
        "print('Relative topic importance in new document:\\n', topic_components)\n",
        "print('Number of previously unseen words in new document:', number_of_new_words)\n",
        "print('Previously unseen words in new document:', new_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Relative topic importance in new document:\n",
            " [0.03228734 0.02794212 0.09324692 0.08562618 0.0295061  0.13685694\n",
            " 0.03450409 0.04901984 0.14699766 0.0283752  0.02069639 0.03340841\n",
            " 0.02356898 0.02509774 0.02469394 0.02015825 0.12767861 0.01704737\n",
            " 0.03311987 0.01016806]\n",
            "Number of previously unseen words in new document: 0\n",
            "Previously unseen words in new document: ()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1UYmn5TMrn"
      },
      "source": [
        "tokenized_corpus = []\n",
        "remove_index = []\n",
        "\n",
        "for i, doc in enumerate(final_articles_list):\n",
        "    if isinstance(doc, str):\n",
        "        tokenized_corpus.append(doc.split())\n",
        "    else:\n",
        "        remove_index.append(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvM-fn_Qb2zG",
        "outputId": "88d45627-810e-4e67-9e00-bfa163f5604a"
      },
      "source": [
        "len(remove_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWAaZ_cEbEXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6518f3-865c-4e4e-a195-a61ccff2410a"
      },
      "source": [
        "for doc in final_articles_list:\n",
        "    if not isinstance(doc, str):\n",
        "        print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n",
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iygysq01aAwh"
      },
      "source": [
        "for doc in remove_index:\n",
        "    final_articles_list.remove(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kyDAT4gaU63",
        "outputId": "8dad8059-8180-4dd6-c1c7-0027ee4ad858"
      },
      "source": [
        "print(len(tokenized_corpus))\n",
        "print(len(final_articles_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149998\n",
            "149998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGzCoTzGA15T"
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "#corpus_ = docs_BN.detach().cpu().numpy() # .cpu().numpy().arra\n",
        "#corpus_ model=lda_model\n",
        "dct = Dictionary(tokenized_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJZ9782-z2u8",
        "outputId": "3bdcb3d1-de8a-43af-8d64-647c93450793"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "# from gensim.corpora import Dictionary\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(topics=topics_prod, texts=tokenized_corpus, dictionary=dct, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.42991018176544626\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}